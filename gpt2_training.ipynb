{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr6YQAnzDKUIlbmN4qV7Cq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahiliem/colab/blob/main/gpt2_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36XtmwxwIOnf",
        "outputId": "a5a4bad0-86cb-4a70-8296-e18d4a8aea64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.27.4\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.4) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.4) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.4) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.4) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.4) (3.10.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.4) (1.22.4)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.27.4) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.4) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.4) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.4) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.27.4) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece!=\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement 0.1.92 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for 0.1.92\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (3.20.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.11.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.27.4\n",
        "!pip install accelerate>=0.12.0\n",
        "!pip install torch>=1.3\n",
        "!pip install datasets>=1.8.0\n",
        "!pip install sentencepiece!=0.1.92\n",
        "!pip install protobuf\n",
        "!pip install evaluate\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sahiliem/all_gpt_finetunings.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3CsTYR0I_rb",
        "outputId": "8d110076-e8f5-4df5-b452-c5fee062ecba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'all_gpt_finetunings'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), 8.12 KiB | 4.06 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9b8dreRJM-E",
        "outputId": "d1285e76-9ddf-494d-da81-3f65de82c8b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README\ttraining.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG7KG94wJVJd",
        "outputId": "93e74932-a0ce-4e2b-bf20-85d53ebe4af6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=0380e38385117aff75665b69baf2475f544bbef63bce5ed140f1e470852a87d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDPNPdJmJhv_",
        "outputId": "65932d2b-733a-46af-9fb0-bea9592d86ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python training.py --cache_dir ./download_models --model_name_or_path gpt2 --train_file /content/all_gpt_finetunings/rio.txt --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --output_dir ./test2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KBh0PVJJryA",
        "outputId": "79eab6c7-cbaf-47a1-9aed-d5acdb601478"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-08 18:37:17.455810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/08/2023 18:37:25 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/08/2023 18:37:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./test2/runs/Apr08_18-37-20_1208cb9ac85e,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./test2,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./test2,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/08/2023 18:37:26 - INFO - datasets.builder - Using custom data configuration default-fdb10143c8df9887\n",
            "04/08/2023 18:37:26 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "04/08/2023 18:37:26 - INFO - datasets.builder - Generating dataset text (/content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "Downloading and preparing dataset text/default to /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 5614.86it/s]\n",
            "04/08/2023 18:37:26 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "04/08/2023 18:37:26 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 971.80it/s]\n",
            "04/08/2023 18:37:26 - INFO - datasets.builder - Generating train split\n",
            "04/08/2023 18:37:26 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 596.37it/s]\n",
            "04/08/2023 18:37:27 - INFO - datasets.builder - Using custom data configuration default-fdb10143c8df9887\n",
            "04/08/2023 18:37:27 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "04/08/2023 18:37:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "04/08/2023 18:37:27 - INFO - datasets.info - Loading Dataset info from ./download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "04/08/2023 18:37:27 - WARNING - datasets.builder - Found cached dataset text (/content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "04/08/2023 18:37:27 - INFO - datasets.info - Loading Dataset info from /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "04/08/2023 18:37:28 - INFO - datasets.builder - Using custom data configuration default-fdb10143c8df9887\n",
            "04/08/2023 18:37:28 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "04/08/2023 18:37:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "04/08/2023 18:37:28 - INFO - datasets.info - Loading Dataset info from ./download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "04/08/2023 18:37:28 - WARNING - datasets.builder - Found cached dataset text (/content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "04/08/2023 18:37:28 - INFO - datasets.info - Loading Dataset info from /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "Downloading (‚Ä¶)lve/main/config.json: 100% 665/665 [00:00<00:00, 114kB/s]\n",
            "[INFO|configuration_utils.py:668] 2023-04-08 18:37:28,590 >> loading configuration file config.json from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-08 18:37:28,591 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.27.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:479] 2023-04-08 18:37:28,830 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:668] 2023-04-08 18:37:29,073 >> loading configuration file config.json from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-08 18:37:29,074 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.27.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading (‚Ä¶)olve/main/vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.24MB/s]\n",
            "Downloading (‚Ä¶)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 718kB/s]\n",
            "Downloading (‚Ä¶)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.30MB/s]\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-04-08 18:37:33,903 >> loading file vocab.json from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-04-08 18:37:33,903 >> loading file merges.txt from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-04-08 18:37:33,903 >> loading file tokenizer.json from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-04-08 18:37:33,903 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-04-08 18:37:33,903 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-04-08 18:37:33,903 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:668] 2023-04-08 18:37:33,903 >> loading configuration file config.json from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-04-08 18:37:33,904 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.27.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 548M/548M [00:01<00:00, 312MB/s]\n",
            "[INFO|modeling_utils.py:2403] 2023-04-08 18:37:35,980 >> loading weights file pytorch_model.bin from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-04-08 18:37:36,759 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.27.4\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3034] 2023-04-08 18:37:39,271 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:3042] 2023-04-08 18:37:39,271 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Downloading (‚Ä¶)neration_config.json: 100% 124/124 [00:00<00:00, 16.8kB/s]\n",
            "[INFO|configuration_utils.py:537] 2023-04-08 18:37:39,781 >> loading configuration file generation_config.json from cache at ./download_models/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
            "[INFO|configuration_utils.py:575] 2023-04-08 18:37:39,782 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.27.4\"\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/223 [00:00<?, ? examples/s]04/08/2023 18:37:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-75ca677b8b31e014.arrow\n",
            "Running tokenizer on dataset:   0% 0/12 [00:00<?, ? examples/s]04/08/2023 18:37:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-dbb329470c5601c3.arrow\n",
            "Grouping texts in chunks of 1024:   0% 0/223 [00:00<?, ? examples/s]04/08/2023 18:37:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-d442207b3280b26a.arrow\n",
            "Grouping texts in chunks of 1024:   0% 0/12 [00:00<?, ? examples/s]04/08/2023 18:37:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /content/all_gpt_finetunings/download_models/text/default-fdb10143c8df9887/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-a3171b3fa4cdd6f8.arrow\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 4.61MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1740] 2023-04-08 18:37:41,546 >> ***** Running training *****\n",
            "[INFO|trainer.py:1741] 2023-04-08 18:37:41,546 >>   Num examples = 9\n",
            "[INFO|trainer.py:1742] 2023-04-08 18:37:41,546 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1743] 2023-04-08 18:37:41,546 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1744] 2023-04-08 18:37:41,546 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1745] 2023-04-08 18:37:41,547 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1746] 2023-04-08 18:37:41,547 >>   Total optimization steps = 6\n",
            "[INFO|trainer.py:1747] 2023-04-08 18:37:41,547 >>   Number of trainable parameters = 124439808\n",
            "[INFO|integrations.py:709] 2023-04-08 18:37:41,559 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmail-analsarkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/all_gpt_finetunings/wandb/run-20230408_183743-6hxrkxbg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgraceful-moon-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/mail-analsarkar/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/mail-analsarkar/huggingface/runs/6hxrkxbg\u001b[0m\n",
            "  0% 0/6 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/all_gpt_finetunings/training.py\", line 620, in <module>\n",
            "    main()\n",
            "  File \"/content/all_gpt_finetunings/training.py\", line 568, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1633, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 2645, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1075, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 899, in forward\n",
            "    outputs = block(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 389, in forward\n",
            "    attn_outputs = self.attn(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 330, in forward\n",
            "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 185, in _attn\n",
            "    attn_weights = attn_weights / torch.full(\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 14.75 GiB total capacity; 13.71 GiB already allocated; 210.81 MiB free; 13.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\u001b[31m‚ï≠‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚ïÆ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/content/all_gpt_finetunings/\u001b[0m\u001b[1;33mtraining.py\u001b[0m:\u001b[94m620\u001b[0m in \u001b[92m<module>\u001b[0m                     \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m617 \u001b[0m                                                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m618 \u001b[0m                                                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m619 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m620 \u001b[2m‚îÇ   \u001b[0mmain()                                                             \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m621 \u001b[0m                                                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/content/all_gpt_finetunings/\u001b[0m\u001b[1;33mtraining.py\u001b[0m:\u001b[94m568\u001b[0m in \u001b[92mmain\u001b[0m                         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m565 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mcheckpoint = training_args.resume_from_checkpoint          \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m566 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melif\u001b[0m last_checkpoint \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m567 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mcheckpoint = last_checkpoint                               \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m568 \u001b[2m‚îÇ   ‚îÇ   \u001b[0mtrain_result = trainer.train(resume_from_checkpoint=checkpoint \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mtrainer.save_model()  \u001b[2m# Saves the tokenizer too for easy uploa\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m570 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                               \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m571 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mmetrics = train_result.metrics                                 \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1633\u001b[0m in \u001b[92mtrain\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1630 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0minner_training_loop = find_executable_batch_size(             \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1631 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.a \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1632 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m)                                                             \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1633 \u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                   \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1634 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0margs=args,                                                \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1635 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,            \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1636 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mtrial=trial,                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1902\u001b[0m in       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1899 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mwith\u001b[0m model.no_sync():                             \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1900 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inpu \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1901 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1902 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)  \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1903 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m                                                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1904 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m (                                                  \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1905 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0margs.logging_nan_inf_filter                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2645\u001b[0m in       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[92mtraining_step\u001b[0m                                                                \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2642 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m loss_mb.reduce_mean().detach().to(\u001b[96mself\u001b[0m.args.device \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2643 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2644 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.compute_loss_context_manager():                     \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m2645 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mloss = \u001b[96mself\u001b[0m.compute_loss(model, inputs)                   \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2646 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2647 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.n_gpu > \u001b[94m1\u001b[0m:                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2648 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mloss = loss.mean()  \u001b[2m# mean() to average on multi-gpu para\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2677\u001b[0m in       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[92mcompute_loss\u001b[0m                                                                 \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2674 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mlabels = inputs.pop(\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m\"\u001b[0m)                             \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2675 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2676 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mlabels = \u001b[94mNone\u001b[0m                                             \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m2677 \u001b[2m‚îÇ   ‚îÇ   \u001b[0moutputs = model(**inputs)                                     \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2678 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# Save past state if it exists\u001b[0m                                \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2679 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# TODO: this needs to be fixed and made cleaner later.\u001b[0m        \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m2680 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.past_index >= \u001b[94m0\u001b[0m:                                 \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in    \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                   \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1501 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mbackward_pre_hooks = []                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[1;33m2.py\u001b[0m:\u001b[94m1075\u001b[0m in \u001b[92mforward\u001b[0m                                                         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1072 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                           \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1073 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96msel\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1074 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1075 \u001b[2m‚îÇ   ‚îÇ   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1076 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0minput_ids,                                                \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1077 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mpast_key_values=past_key_values,                          \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1078 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mattention_mask=attention_mask,                            \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in    \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                   \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1501 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mbackward_pre_hooks = []                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[1;33m2.py\u001b[0m:\u001b[94m899\u001b[0m in \u001b[92mforward\u001b[0m                                                          \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 896 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mencoder_attention_mask,                           \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 897 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m)                                                     \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 898 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                     \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m 899 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0moutputs = block(                                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 900 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mhidden_states,                                    \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 901 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mlayer_past=layer_past,                            \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 902 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mattention_mask=attention_mask,                    \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in    \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                   \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1501 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mbackward_pre_hooks = []                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[1;33m2.py\u001b[0m:\u001b[94m389\u001b[0m in \u001b[92mforward\u001b[0m                                                          \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 386 \u001b[0m\u001b[2m‚îÇ   \u001b[0m) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tupl \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 387 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mresidual = hidden_states                                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 388 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.ln_1(hidden_states)                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m 389 \u001b[2m‚îÇ   ‚îÇ   \u001b[0mattn_outputs = \u001b[96mself\u001b[0m.attn(                                     \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 390 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mhidden_states,                                            \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 391 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mlayer_past=layer_past,                                    \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 392 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mattention_mask=attention_mask,                            \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in    \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                   \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96ms\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hoo \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m1501 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mbackward_pre_hooks = []                                       \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[1;33m2.py\u001b[0m:\u001b[94m330\u001b[0m in \u001b[92mforward\u001b[0m                                                          \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 327 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.reorder_and_upcast_attn:                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 328 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._upcast_and_reordered_at \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 329 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m 330 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._attn(query, key, value, \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 331 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 332 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mattn_output = \u001b[96mself\u001b[0m._merge_heads(attn_output, \u001b[96mself\u001b[0m.num_heads,  \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 333 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mattn_output = \u001b[96mself\u001b[0m.c_proj(attn_output)                        \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt\u001b[0m \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[1;33m2.py\u001b[0m:\u001b[94m185\u001b[0m in \u001b[92m_attn\u001b[0m                                                            \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m                                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 182 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mattn_weights = torch.matmul(query, key.transpose(-\u001b[94m1\u001b[0m, -\u001b[94m2\u001b[0m))     \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 183 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m                                                              \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 184 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.scale_attn_weights:                                   \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m 185 \u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0mattn_weights = attn_weights / torch.full(                 \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 186 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m[], value.size(-\u001b[94m1\u001b[0m) ** \u001b[94m0.5\u001b[0m, dtype=attn_weights.dtype,  \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 187 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m)                                                         \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 188 \u001b[0m                                                                      \u001b[31m‚îÇ\u001b[0m\n",
            "\u001b[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
            "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m384.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.75\u001b[0m\n",
            "GiB total capacity; \u001b[1;36m13.71\u001b[0m GiB already allocated; \u001b[1;36m210.81\u001b[0m MiB free; \u001b[1;36m13.77\u001b[0m GiB \n",
            "reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
            "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \n",
            "Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mgraceful-moon-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/mail-analsarkar/huggingface/runs/6hxrkxbg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230408_183743-6hxrkxbg/logs\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}