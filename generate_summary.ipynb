{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBFjv3JqjxuT0d8yHadnI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahiliem/colab/blob/main/generate_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pjGiHkMpP97",
        "outputId": "e3d2232b-80ff-4364-a39d-374c313be10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8R5mW0kpkIi",
        "outputId": "878378fa-d40e-4c80-a842-47722c4d2f63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input text\n",
        "input_text = '''You are AI assistant who is expert in Iron mining related production planning and forcasting analysis. Do provide insights based on the below points. Compare and provide commentry for actual vs plan at each mine level with insights. Avoid asumptions.\n",
        "\n",
        "\t\tCurrent Month\t\t\t\tYTD\t\t\n",
        "\t% of Plan\tActual\tPlan\t\t% of Plan\tActual\tPlan\tFY Plan\n",
        "\t\t\t\t\t\t\t\t\n",
        "\t\t\t\t\t\t\t\t\n",
        "Mine 1\t85%\t19,29,141\t22,71,533\t\t97%\t2,33,43,469\t2,41,06,984\t0\n",
        "Mine 2\t82%\t17,18,749\t20,88,513\t\t99%\t1,81,14,971\t1,83,19,119\t0\n",
        "Mine 3\t81%\t8,82,446\t10,89,000\t\t98%\t1,28,75,730\t1,31,61,265\t0\n",
        "Mine 4\t49%\t5,54,634\t11,32,541\t\t32%\t41,84,501\t1,31,90,106\t0\n",
        "Mine 5\t120%\t39,01,889\t32,44,804\t\t100%\t4,23,60,767\t4,22,65,651\t0\n",
        "Mine 6\t0%\t89,709\t0\t\t0%\t69,57,214\t0\t0\n",
        "Mine 7\t120%\t20,03,583\t16,73,563\t\t105%\t3,15,60,902\t3,01,38,586\t0\n",
        "Mine 8\t105%\t52,56,631\t49,86,164\t\t99%\t5,66,49,770\t5,70,53,703\t0\n",
        "Mine 9\t99%\t29,16,292\t29,36,543\t\t95%\t3,14,34,649\t3,31,08,281\t0\n",
        "Mine 10\t105%\t26,20,845\t24,94,269\t\t102%\t3,09,89,026\t3,05,00,271\t0\n",
        "Mine 11\t120%\t19,44,953\t16,25,000\t\t99%\t1,78,60,930\t1,80,05,750\t0\n",
        "Mine 12\t105%\t30,40,351\t28,87,220\t\t82%\t2,55,57,667\t3,12,91,499\t0\n",
        "Mine 13\t94%\t39,50,060\t42,24,459\t\t81%\t2,22,56,250\t2,75,57,026\t0\n",
        "\t101%\t3,08,09,283\t3,06,53,608\t\t96%\t32,41,45,846\t33,86,98,242\t0'''\n"
      ],
      "metadata": {
        "id": "SKJ-LVSzpgEi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate summary\n",
        "summary_ids = model.generate(input_ids, num_beams=4, max_length=512,early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Print summary\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-rxV3cKqG95",
        "outputId": "f431e917-18f2-4df0-f15b-ca8bc6aa2dd4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan % of Plan Actual Plan FY Plan % of Plan Actual Plan % of Plan Actual Plan % of Plan Actual Plan\n"
          ]
        }
      ]
    }
  ]
}